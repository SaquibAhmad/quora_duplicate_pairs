{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import warnings\n",
    "import networkx as nx\n",
    "from pandas.core.common import SettingWithCopyWarning\n",
    "warnings.simplefilter('ignore', SettingWithCopyWarning)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Input \n",
    "from keras.layers import Lambda \n",
    "from keras.layers import LSTM, Bidirectional\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "from keras.models import Model\n",
    "import keras.backend as K\n",
    "from keras.optimizers import Adadelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv')\n",
    "df = df[['question1', 'question2', 'is_duplicate']]\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_val = train_test_split(df, random_state=42, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(x):\n",
    "    x = str(x).lower()\n",
    "    x = x.replace(\"won't\", \"will not\")\n",
    "    x = x.replace(\"cannot\", \"can not\")\n",
    "    x = x.replace(\"can't\", \"can not\")\n",
    "    x = x.replace(\"n't\", \" not\")\n",
    "    x = x.replace(\"what's\", \"what is\")\n",
    "    x = x.replace(\"it's\", \"it is\")\n",
    "    x = x.replace(\"'ve\", \" have\")\n",
    "    x = x.replace(\"i'm\", \"i am\")\n",
    "    x = x.replace(\"'re\", \" are\")\n",
    "    x = x.replace(\"he's\", \"he is\")\n",
    "    x = x.replace(\"she's\", \"she is\")\n",
    "    x = x.replace(\"'s\", \" own\")\n",
    "    x = x.replace(\"%\", \" percent \")\n",
    "    x = x.replace(\"₹\", \" rupee \")\n",
    "    x = x.replace(\"$\", \" dollar \")\n",
    "    x = x.replace(\"€\", \" euro \")\n",
    "    x = x.replace(\"'ll\", \" will\")\n",
    "    x = x.strip()\n",
    "    x = ' '.join(x.split())\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['question1'] = df_train['question1'].apply(preprocess)\n",
    "df_train['question2'] = df_train['question2'].apply(preprocess)\n",
    "df_train = df_train[(df_train.question1!='') & (df_train.question2!='')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val['question1'] = df_val['question1'].apply(preprocess)\n",
    "df_val['question2'] = df_val['question2'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre Trained Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenizer(df):\n",
    "    \n",
    "    tokenizer =  Tokenizer()\n",
    "    question_text_list = df['question1'].astype(str).tolist() + df['question2'].astype(str).tolist()\n",
    "    tokenizer.fit_on_texts(question_text_list)\n",
    "    \n",
    "    return tokenizer\n",
    "\n",
    "tokenizer = get_tokenizer(df_train)\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare input and output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_X_y(df, max_length, tokenizer):\n",
    "    \n",
    "    encoded_question1 = tokenizer.texts_to_sequences(df['question1'].astype(str).tolist())\n",
    "    encoded_question2 = tokenizer.texts_to_sequences(df['question2'].astype(str).tolist())\n",
    "    padded_question1 = pad_sequences(encoded_question1, maxlen=max_length, padding='post')\n",
    "    padded_question2 = pad_sequences(encoded_question2, maxlen=max_length, padding='post')\n",
    "    \n",
    "    X_1 = padded_question1\n",
    "    X_2 = padded_question2\n",
    "    y = df.is_duplicate.values\n",
    "    \n",
    "    return X_1, X_2, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_1, X_train_2, y_train = prepare_X_y(df_train, max_length=MAX_LENGTH, tokenizer=tokenizer)\n",
    "X_val_1, X_val_2, y_val = prepare_X_y(df_val, max_length=MAX_LENGTH, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_matrix(tokenizer, vector_model):\n",
    "    \n",
    "    embedding_matrix = np.zeros((vocab_size, 300))\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        if word in vector_model.vocab:\n",
    "            embedding_vector = vector_model.word_vec(word)\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            \n",
    "    return embedding_matrix\n",
    "\n",
    "embedding_matrix = get_embedding_matrix(tokenizer, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Siamese Model with 1D CNN and Bidirectional LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(embedding_matrix):\n",
    "    \n",
    "    input_1 = Input(shape=(MAX_LENGTH,), dtype='int32')\n",
    "    input_2 = Input(shape=(MAX_LENGTH,), dtype='int32')\n",
    "\n",
    "    embedding_layer = Embedding(len(embedding_matrix),\n",
    "                                300, weights=[embedding_matrix],\n",
    "                                input_length=MAX_LENGTH,\n",
    "                                trainable=False)\n",
    "\n",
    "    encoded_1 = embedding_layer(input_1)\n",
    "    encoded_2 = embedding_layer(input_2)\n",
    "\n",
    "    shared_conv = Conv1D(filters=128, kernel_size=3, padding='same', activation='relu')\n",
    "    max_pool = MaxPooling1D(pool_size=2)\n",
    "    shared_lstm = Bidirectional(LSTM(256))\n",
    "\n",
    "    output_1 = shared_conv(encoded_1)\n",
    "    output_1 = max_pool(output_1)\n",
    "    output_1 = shared_lstm(output_1)\n",
    "    \n",
    "    output_2 = shared_conv(encoded_2)\n",
    "    output_2 = max_pool(output_2)\n",
    "    output_2 = shared_lstm(output_2)\n",
    "\n",
    "    dist = Lambda(function=lambda x: K.exp(-K.sum(K.abs(x[0]-x[1]), axis=1, keepdims=True)),\n",
    "                             output_shape=lambda x: (x[0][0], 1))([output_1, output_2])\n",
    "\n",
    "    # Pack it all up into a model\n",
    "    model = Model([input_1, input_2], [dist])\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    print(model.summary())\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = get_model(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit([X_train_1, X_train_2], y_train, batch_size=256, epochs=8, validation_data=([X_val_1, X_val_2], y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('models/tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "\n",
    "model.save_weights('models/model_weights.h5')\n",
    "\n",
    "with open('models/model_architecture.json', 'w') as f:\n",
    "    f.write(model.to_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import warnings\n",
    "import networkx as nx\n",
    "from pandas.core.common import SettingWithCopyWarning\n",
    "warnings.simplefilter('ignore', SettingWithCopyWarning)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Input \n",
    "from keras.layers import Lambda \n",
    "from keras.layers import LSTM, Bidirectional\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "from keras.models import Model\n",
    "import keras.backend as K\n",
    "from keras.optimizers import Adadelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv')\n",
    "df = df[['question1', 'question2', 'is_duplicate']]\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_val = train_test_split(df, random_state=42, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(x):\n",
    "    x = str(x).lower()\n",
    "    x = x.replace(\"won't\", \"will not\")\n",
    "    x = x.replace(\"cannot\", \"can not\")\n",
    "    x = x.replace(\"can't\", \"can not\")\n",
    "    x = x.replace(\"n't\", \" not\")\n",
    "    x = x.replace(\"what's\", \"what is\")\n",
    "    x = x.replace(\"it's\", \"it is\")\n",
    "    x = x.replace(\"'ve\", \" have\")\n",
    "    x = x.replace(\"i'm\", \"i am\")\n",
    "    x = x.replace(\"'re\", \" are\")\n",
    "    x = x.replace(\"he's\", \"he is\")\n",
    "    x = x.replace(\"she's\", \"she is\")\n",
    "    x = x.replace(\"'s\", \" own\")\n",
    "    x = x.replace(\"%\", \" percent \")\n",
    "    x = x.replace(\"₹\", \" rupee \")\n",
    "    x = x.replace(\"$\", \" dollar \")\n",
    "    x = x.replace(\"€\", \" euro \")\n",
    "    x = x.replace(\"'ll\", \" will\")\n",
    "    x = x.strip()\n",
    "    x = ' '.join(x.split())\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['question1'] = df_train['question1'].apply(preprocess)\n",
    "df_train['question2'] = df_train['question2'].apply(preprocess)\n",
    "df_train = df_train[(df_train.question1!='') & (df_train.question2!='')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val['question1'] = df_val['question1'].apply(preprocess)\n",
    "df_val['question2'] = df_val['question2'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre Trained Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenizer(df):\n",
    "    \n",
    "    tokenizer =  Tokenizer()\n",
    "    question_text_list = df['question1'].astype(str).tolist() + df['question2'].astype(str).tolist()\n",
    "    tokenizer.fit_on_texts(question_text_list)\n",
    "    \n",
    "    return tokenizer\n",
    "\n",
    "tokenizer = get_tokenizer(df_train)\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare input and output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_X_y(df, max_length, tokenizer):\n",
    "    \n",
    "    encoded_question1 = tokenizer.texts_to_sequences(df['question1'].astype(str).tolist())\n",
    "    encoded_question2 = tokenizer.texts_to_sequences(df['question2'].astype(str).tolist())\n",
    "    padded_question1 = pad_sequences(encoded_question1, maxlen=max_length, padding='post')\n",
    "    padded_question2 = pad_sequences(encoded_question2, maxlen=max_length, padding='post')\n",
    "    \n",
    "    X_1 = padded_question1\n",
    "    X_2 = padded_question2\n",
    "    y = df.is_duplicate.values\n",
    "    \n",
    "    return X_1, X_2, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_1, X_train_2, y_train = prepare_X_y(df_train, max_length=MAX_LENGTH, tokenizer=tokenizer)\n",
    "X_val_1, X_val_2, y_val = prepare_X_y(df_val, max_length=MAX_LENGTH, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_matrix(tokenizer, vector_model):\n",
    "    \n",
    "    embedding_matrix = np.zeros((vocab_size, 300))\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        if word in vector_model.vocab:\n",
    "            embedding_vector = vector_model.word_vec(word)\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            \n",
    "    return embedding_matrix\n",
    "\n",
    "embedding_matrix = get_embedding_matrix(tokenizer, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Siamese Model with 1D CNN and Bidirectional LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 50)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 50)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 50, 300)      23759100    input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 50, 128)      115328      embedding_1[0][0]                \n",
      "                                                                 embedding_1[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 25, 128)      0           conv1d_1[0][0]                   \n",
      "                                                                 conv1d_1[1][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 512)          788480      max_pooling1d_1[0][0]            \n",
      "                                                                 max_pooling1d_1[1][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 1)            0           bidirectional_1[0][0]            \n",
      "                                                                 bidirectional_1[1][0]            \n",
      "==================================================================================================\n",
      "Total params: 24,662,908\n",
      "Trainable params: 903,808\n",
      "Non-trainable params: 23,759,100\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def get_model(embedding_matrix):\n",
    "    \n",
    "    input_1 = Input(shape=(MAX_LENGTH,), dtype='int32')\n",
    "    input_2 = Input(shape=(MAX_LENGTH,), dtype='int32')\n",
    "\n",
    "    embedding_layer = Embedding(len(embedding_matrix),\n",
    "                                300, weights=[embedding_matrix],\n",
    "                                input_length=MAX_LENGTH,\n",
    "                                trainable=False)\n",
    "\n",
    "    encoded_1 = embedding_layer(input_1)\n",
    "    encoded_2 = embedding_layer(input_2)\n",
    "\n",
    "    shared_conv = Conv1D(filters=128, kernel_size=3, padding='same', activation='relu')\n",
    "    max_pool = MaxPooling1D(pool_size=2)\n",
    "    shared_lstm = Bidirectional(LSTM(256))\n",
    "\n",
    "    output_1 = shared_conv(encoded_1)\n",
    "    output_1 = max_pool(output_1)\n",
    "    output_1 = shared_lstm(output_1)\n",
    "    \n",
    "    output_2 = shared_conv(encoded_2)\n",
    "    output_2 = max_pool(output_2)\n",
    "    output_2 = shared_lstm(output_2)\n",
    "\n",
    "    dist = Lambda(function=lambda x: K.exp(-K.sum(K.abs(x[0]-x[1]), axis=1, keepdims=True)),\n",
    "                             output_shape=lambda x: (x[0][0], 1))([output_1, output_2])\n",
    "\n",
    "    # Pack it all up into a model\n",
    "    model = Model([input_1, input_2], [dist])\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    print(model.summary())\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = get_model(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 291086 samples, validate on 72772 samples\n",
      "Epoch 1/8\n",
      "291086/291086 [==============================] - 284s 975us/step - loss: 0.6221 - acc: 0.7425 - val_loss: 0.5731 - val_acc: 0.7696\n",
      "Epoch 2/8\n",
      "291086/291086 [==============================] - 279s 957us/step - loss: 0.5687 - acc: 0.7821 - val_loss: 0.5548 - val_acc: 0.7783\n",
      "Epoch 3/8\n",
      "291086/291086 [==============================] - 279s 958us/step - loss: 0.5397 - acc: 0.8025 - val_loss: 0.5444 - val_acc: 0.7843\n",
      "Epoch 4/8\n",
      "291086/291086 [==============================] - 279s 958us/step - loss: 0.5180 - acc: 0.8158 - val_loss: 0.5338 - val_acc: 0.7945\n",
      "Epoch 5/8\n",
      "291086/291086 [==============================] - 279s 958us/step - loss: 0.5010 - acc: 0.8266 - val_loss: 0.5314 - val_acc: 0.7949\n",
      "Epoch 6/8\n",
      "291086/291086 [==============================] - 278s 956us/step - loss: 0.4861 - acc: 0.8366 - val_loss: 0.5274 - val_acc: 0.7993\n",
      "Epoch 7/8\n",
      "291086/291086 [==============================] - 280s 960us/step - loss: 0.4743 - acc: 0.8425 - val_loss: 0.5245 - val_acc: 0.8008\n",
      "Epoch 8/8\n",
      "291086/291086 [==============================] - 280s 961us/step - loss: 0.4632 - acc: 0.8490 - val_loss: 0.5273 - val_acc: 0.7999\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f81229b5c88>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([X_train_1, X_train_2], y_train, batch_size=256, epochs=8, validation_data=([X_val_1, X_val_2], y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('models/tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "\n",
    "model.save_weights('models/model_weights.h5')\n",
    "\n",
    "with open('models/model_architecture.json', 'w') as f:\n",
    "    f.write(model.to_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
